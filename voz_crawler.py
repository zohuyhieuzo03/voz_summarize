import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from datetime import datetime
import google.generativeai as genai
import os
from dotenv import load_dotenv
import re
from urllib.parse import urlparse, parse_qs, unquote

# Load environment variables
load_dotenv()

genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
model = genai.GenerativeModel('gemini-2.0-flash')

def get_total_pages(soup):
    page_numbers = soup.find_all("li", class_="pageNav-page")
    if page_numbers:
        last_page = page_numbers[-1].find("a")
        if last_page:
            return int(last_page.text.strip())
    return 1

def get_top_comments(url, num_comments=5, progress_callback=None):
    # Ch·ªâ l·∫•y ph·∫ßn ƒë·∫ßu url ƒë·∫øn d·∫°ng voz.vn/t/...
    match = re.match(r"^(https?://voz.vn/t/[^/]+\.\d+)", url)
    if match:
        url = match.group(1)
    headers = {"User-Agent": "Mozilla/5.0"}
    page = 1
    top_comments = []
    news_content = None
    news_title = None
    full_url = url
    response = requests.get(full_url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    total_pages = get_total_pages(soup)
    # L·∫•y title t·ª´ <h1 class="p-title-value">
    title_tag = soup.find("h1", class_="p-title-value")
    if title_tag:
        news_title = title_tag.get_text(strip=True)
    if progress_callback:
        progress_callback(0, total_pages)
    while True:
        full_url = url if page == 1 else url.rstrip("/") + f"/page-{page}"
        response = requests.get(full_url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")
        posts = soup.find_all("article", {"class": "message"})
        for post in posts:
            if page == 1 and news_content is None:
                content_div = post.find("div", class_="message-content")
                if content_div:
                    # S·ª≠a ·∫£nh proxy th√†nh link g·ªëc
                    html = content_div.decode_contents()
                    soup2 = BeautifulSoup(html, "html.parser")
                    for img in soup2.find_all("img"):
                        if img.has_attr("data-url") and img["data-url"]:
                            img["src"] = img["data-url"]
                        elif img.has_attr("data-src") and img["data-src"]:
                            img["src"] = img["data-src"]
                        elif img.has_attr("src"):
                            src = img["src"]
                            if src.startswith("/proxy.php"):
                                qs = parse_qs(urlparse(src).query)
                                if "url" in qs:
                                    img["src"] = unquote(qs["url"][0])
                            elif src.startswith("/attachments/"):
                                img["src"] = "https://voz.vn" + src
                    news_content = str(soup2)
            react_tag = post.find("a", class_="reactionsBar-link")
            if react_tag:
                reacts_text = react_tag.get_text()
                visible_reacts = len(react_tag.find_all("bdi"))
                if "and" in reacts_text and "others" in reacts_text:
                    others_text = reacts_text.split("and")[1].split("others")[0].strip()
                    try:
                        others_count = int(others_text)
                        react_count = visible_reacts + others_count
                    except ValueError:
                        react_count = visible_reacts
                else:
                    react_count = visible_reacts
                content_div = post.find("div", class_="message-content")
                reaction_ul = post.find("ul", class_="reactionSummary")
                is_pos = is_positive_comment(reaction_ul)
                date_str = None
                date_obj = None
                time_tag = post.find("time", class_="u-dt")
                if time_tag and time_tag.has_attr("datetime"):
                    date_str = time_tag["datetime"]
                    try:
                        date_obj = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
                    except Exception:
                        date_obj = None
                if content_div:
                    html = content_div.decode_contents()
                    soup2 = BeautifulSoup(html, "html.parser")
                    for img in soup2.find_all("img"):
                        if img.has_attr("data-url") and img["data-url"]:
                            img["src"] = img["data-url"]
                        elif img.has_attr("data-src") and img["data-src"]:
                            img["src"] = img["data-src"]
                        elif img.has_attr("src"):
                            src = img["src"]
                            if src.startswith("/proxy.php"):
                                qs = parse_qs(urlparse(src).query)
                                if "url" in qs:
                                    img["src"] = unquote(qs["url"][0])
                            elif src.startswith("/attachments/"):
                                img["src"] = "https://voz.vn" + src
                    for img in soup2.find_all("img"):
                        img["style"] = "max-width:400px; height:auto;"  # ho·∫∑c img["width"] = "400"
                    content = str(soup2)
                    comment = {
                        "reacts": react_count,
                        "text": content,
                        "link": post["itemid"] if "itemid" in post.attrs else full_url + f"#post-{post['data-content']}",
                        "is_positive": is_pos,
                        "date": date_obj
                    }
                    top_comments.append(comment)
        if progress_callback:
            progress_callback(page, total_pages)
        page += 1
        if page > total_pages:
            break
    top_comments.sort(key=lambda x: x["reacts"], reverse=True)
    # count number of comment that react > 20
    num_comments_over_20 = len([c for c in top_comments if c["reacts"] > 20])
    get_number = max(num_comments, num_comments_over_20)
    top_comments = top_comments[:get_number]

    return news_title, news_content, top_comments

def analyze_content_with_gemini(news_content, comments):
    prompt = """H√£y ph√¢n t√≠ch n·ªôi dung c·ªßa b√†i b√°o d∆∞·ªõi ƒë√¢y v√† k·∫øt h·ª£p v·ªõi c√°c b√¨nh lu·∫≠n c·ªßa ƒë·ªôc gi·∫£ ƒë·ªÉ ƒë∆∞a ra m·ªôt b·∫£n t√≥m t·∫Øt v√† ph√¢n t√≠ch ƒëa chi·ªÅu.

**Y√™u c·∫ßu:**
- S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng Markdown ƒë·ªÉ tr√¨nh b√†y
- T·∫°o c√°c ti√™u ƒë·ªÅ r√µ r√†ng v·ªõi ##
- S·ª≠ d·ª•ng danh s√°ch c√≥ d·∫•u ƒë·∫ßu d√≤ng (-) cho c√°c ƒëi·ªÉm ch√≠nh
- S·ª≠ d·ª•ng **bold** cho c√°c t·ª´ kh√≥a quan tr·ªçng
- S·ª≠ d·ª•ng > cho c√°c tr√≠ch d·∫´n ƒë·∫∑c bi·ªát

**C·∫•u tr√∫c ph√¢n t√≠ch:**

## üì∞ T√≥m t·∫Øt b√†i b√°o
- N·ªôi dung ch√≠nh
- L·∫≠p lu·∫≠n v√† k·∫øt lu·∫≠n

## üí¨ Ph√¢n t√≠ch b√¨nh lu·∫≠n c·ªông ƒë·ªìng
- T·ªïng quan c√°c √Ω ki·∫øn
- Nh·∫≠n ƒë·ªãnh v·ªÅ ch·ªß ƒë·ªÅ

## üîç K·∫øt lu·∫≠n t·ªïng quan
- ƒê√°nh gi√° t·ªïng th·ªÉ v·ªÅ ch·ªß ƒë·ªÅ

D∆∞·ªõi ƒë√¢y l√† n·ªôi dung b√†i b√°o v√† b√¨nh lu·∫≠n:

**N·ªôi dung b√†i b√°o:**
{news_content}

**C√°c b√¨nh lu·∫≠n:**
{comments}"""
    try:
        response = model.generate_content(prompt.format(
            news_content=news_content,
            comments="\n".join([f"B√¨nh lu·∫≠n {i+1} ({c['reacts']} reacts): {c['text']}" for i, c in enumerate(comments)])
        ))
        return response.text
    except Exception as e:
        return f"Error analyzing content with Gemini: {str(e)}"

def get_comments_for_ai_analysis(url, db, Comment, news_id):
    """
    L·∫•y t·∫•t c·∫£ comments ƒë·ªÉ ph√¢n t√≠ch AI (kh√¥ng gi·ªõi h·∫°n s·ªë l∆∞·ª£ng)
    """
    # L·∫•y t·∫•t c·∫£ comments hi·ªán c√≥ trong database
    existing_comments = Comment.query.filter_by(news_id=news_id).all()
    
    # Lu√¥n crawl th√™m comments ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ ƒë·∫ßy ƒë·ªß
    try:
        # Crawl t·∫•t c·∫£ comments c√≥ th·ªÉ (kh√¥ng gi·ªõi h·∫°n s·ªë l∆∞·ª£ng)
        news_title, news_content, additional_comments = get_top_comments(url, num_comments=10000)
        
        # L·∫•y c√°c link comment ƒë√£ c√≥ trong DB
        existing_links = set(c.link for c in existing_comments)
        
        # Th√™m comments m·ªõi v√†o database
        for c in additional_comments:
            if c['link'] not in existing_links:
                comment = Comment(
                    news_id=news_id,
                    reacts=c['reacts'],
                    text=c['text'],
                    link=c['link'],
                    is_positive=c.get('is_positive'),
                    created_at=c.get('date') if c.get('date') else None
                )
                db.session.add(comment)
        
        db.session.commit()
        
        # L·∫•y l·∫°i t·∫•t c·∫£ comments sau khi th√™m m·ªõi
        all_comments = Comment.query.filter_by(news_id=news_id).all()
        return all_comments
    except Exception as e:
        print(f"Error crawling additional comments for AI analysis: {e}")
        return existing_comments

def get_display_comments(db, Comment, news_id):
    """
    L·∫•y comments ƒë·ªÉ hi·ªÉn th·ªã theo rule c≈©:
    - Top 5 comments theo react, sort theo date
    - N·∫øu c√≥ >5 comments v·ªõi >20 react th√¨ l·∫•y h·∫øt
    """
    # L·∫•y t·∫•t c·∫£ comments t·ª´ database
    all_comments = Comment.query.filter_by(news_id=news_id).all()
    
    # L·ªçc comments c√≥ >20 react
    high_react_comments = [c for c in all_comments if c.reacts and c.reacts > 20]
    
    # N·∫øu c√≥ >5 comments v·ªõi >20 react, l·∫•y t·∫•t c·∫£ comments c√≥ >20 react
    if len(high_react_comments) > 5:
        # Sort theo date (oldest first)
        display_comments = sorted(high_react_comments, key=lambda c: c.created_at or datetime.min)
    else:
        # L·∫•y top 5 comments theo react, sort theo date
        sorted_comments = sorted(all_comments, key=lambda c: (c.reacts or 0, c.created_at or datetime.min), reverse=True)
        display_comments = sorted_comments[:5]
        # Sort l·∫°i theo date (oldest first)
        display_comments = sorted(display_comments, key=lambda c: c.created_at or datetime.min)
    
    return display_comments

def truncate_text(text, max_length=8000):
    """
    C·∫Øt text n·∫øu qu√° d√†i, gi·ªØ l·∫°i ph·∫ßn ƒë·∫ßu v√† cu·ªëi
    """
    if len(text) <= max_length:
        return text
    
    # C·∫Øt ·ªü gi·ªØa, gi·ªØ l·∫°i 70% ƒë·∫ßu v√† 30% cu·ªëi
    first_part = int(max_length * 0.7)
    last_part = max_length - first_part
    
    return text[:first_part] + "\n\n[... n·ªôi dung b·ªã c·∫Øt ...]\n\n" + text[-last_part:]

def chunk_comments_for_ai(comments, max_comments_per_chunk=200, max_chars_per_comment=2000):
    """
    Chia comments th√†nh c√°c chunk nh·ªè h∆°n ƒë·ªÉ x·ª≠ l√Ω
    """
    chunks = []
    current_chunk = []
    current_length = 0
    
    for i, comment in enumerate(comments):
        # X·ª≠ l√Ω c·∫£ comment object v√† dict
        if hasattr(comment, 'reacts'):
            reacts = comment.reacts
            text = comment.text
            link = comment.link
        else:
            reacts = comment.get('reacts', 0)
            text = comment.get('text', '')
            link = comment.get('link', '')
        
        # C·∫Øt text comment n·∫øu qu√° d√†i
        if len(text) > max_chars_per_comment:
            text = truncate_text(text, max_chars_per_comment)
        
        comment_text = f"\nB√¨nh lu·∫≠n {i+1} ({reacts} reacts) - Link: {link}:\n{text}\n"
        comment_length = len(comment_text)
        
        # N·∫øu th√™m comment n√†y v∆∞·ª£t qu√° gi·ªõi h·∫°n ho·∫∑c ƒë√£ ƒë·ªß s·ªë l∆∞·ª£ng
        if (current_length + comment_length > 15000) or len(current_chunk) >= max_comments_per_chunk:
            if current_chunk:  # L∆∞u chunk hi·ªán t·∫°i
                chunks.append(current_chunk)
                current_chunk = []
                current_length = 0
        
        current_chunk.append(comment)
        current_length += comment_length
    
    # Th√™m chunk cu·ªëi c√πng
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def chat_with_ai_about_thread(title, content, comments, question, url=None, db=None, Comment=None, news_id=None):
    """
    Chat v·ªõi AI v·ªÅ m·ªôt thread c·ª• th·ªÉ d·ª±a tr√™n title, content v√† comments
    """
    # N·∫øu c√≥ th√¥ng tin database, l·∫•y comments ƒë·ªÉ ph√¢n t√≠ch AI
    if url and db and Comment and news_id:
        ai_comments = get_comments_for_ai_analysis(url, db, Comment, news_id)
        comments = ai_comments
    
    # C·∫Øt content n·∫øu qu√° d√†i
    content = truncate_text(content, 5000)
    
    # Chia comments th√†nh chunks
    comment_chunks = chunk_comments_for_ai(comments)
    
    # N·∫øu ch·ªâ c√≥ 1 chunk ho·∫∑c √≠t comments, x·ª≠ l√Ω b√¨nh th∆∞·ªùng
    if len(comment_chunks) <= 1:
        thread_context = f"""
        Ti√™u ƒë·ªÅ thread: {title}
        
        N·ªôi dung ch√≠nh:
        {content}
        
        C√°c b√¨nh lu·∫≠n t·ª´ c·ªông ƒë·ªìng (t·ªïng c·ªông {len(comments)} b√¨nh lu·∫≠n):
        """
        
        # Th√™m comments v√†o context v·ªõi link
        for i, comment in enumerate(comments):
            if hasattr(comment, 'reacts'):
                reacts = comment.reacts
                text = comment.text
                link = comment.link
            else:
                reacts = comment.get('reacts', 0)
                text = comment.get('text', '')
                link = comment.get('link', '')
            
            # C·∫Øt text comment n·∫øu qu√° d√†i
            if len(text) > 2000:
                text = truncate_text(text, 2000)
            
            thread_context += f"\nB√¨nh lu·∫≠n {i+1} ({reacts} reacts) - Link: {link}:\n{text}\n"
        
        return process_single_chunk(title, content, thread_context, question, len(comments))
    
    # N·∫øu c√≥ nhi·ªÅu chunks, x·ª≠ l√Ω t·ª´ng chunk v√† t·ªïng h·ª£p
    else:
        return process_multiple_chunks(title, content, comment_chunks, question, len(comments))

def process_single_chunk(title, content, thread_context, question, total_comments):
    """
    X·ª≠ l√Ω m·ªôt chunk duy nh·∫•t
    """
    prompt = f"""
    B·∫°n l√† m·ªôt AI assistant chuy√™n ph√¢n t√≠ch v√† tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ c√°c thread tr√™n di·ªÖn ƒë√†n VOZ.
    
    D∆∞·ªõi ƒë√¢y l√† th√¥ng tin v·ªÅ m·ªôt thread c·ª• th·ªÉ:
    
    {thread_context}
    
    C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {question}
    
    **Y√™u c·∫ßu tr·∫£ l·ªùi:**
    - S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng Markdown ƒë·ªÉ tr√¨nh b√†y
    - T·∫°o c√°c ti√™u ƒë·ªÅ r√µ r√†ng v·ªõi ##
    - S·ª≠ d·ª•ng danh s√°ch c√≥ d·∫•u ƒë·∫ßu d√≤ng (-) cho c√°c ƒëi·ªÉm ch√≠nh
    - S·ª≠ d·ª•ng **bold** cho c√°c t·ª´ kh√≥a quan tr·ªçng
    - S·ª≠ d·ª•ng > cho c√°c tr√≠ch d·∫´n t·ª´ b√¨nh lu·∫≠n c·ª• th·ªÉ
    - S·ª≠ d·ª•ng `code` cho c√°c thu·∫≠t ng·ªØ k·ªπ thu·∫≠t
    - **Khi tr√≠ch d·∫´n comment:** S·ª≠ d·ª•ng format: > [N·ªôi dung comment](Link comment)
    
    H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin t·ª´ thread tr√™n. N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn:
    - **S·∫£n ph·∫©m/d·ªãch v·ª•:** H√£y ph√¢n t√≠ch c√°c √Ω ki·∫øn, ƒë√°nh gi√°, khuy·∫øn ngh·ªã t·ª´ c·ªông ƒë·ªìng
    - **S·ª± ki·ªán/tin t·ª©c:** H√£y t√≥m t·∫Øt v√† ph√¢n t√≠ch c√°c g√≥c nh√¨n kh√°c nhau
    - **Kinh nghi·ªám/c√°ch l√†m:** H√£y t·ªïng h·ª£p c√°c chia s·∫ª th·ª±c t·∫ø t·ª´ ng∆∞·ªùi d√πng
    - **So s√°nh/l·ª±a ch·ªçn:** H√£y ƒë∆∞a ra ph√¢n t√≠ch d·ª±a tr√™n c√°c √Ω ki·∫øn trong thread
    
    Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, r√µ r√†ng v√† c√≥ c·∫•u tr√∫c. N·∫øu c√≥ th·ªÉ, h√£y tr√≠ch d·∫´n c√°c b√¨nh lu·∫≠n c·ª• th·ªÉ ƒë·ªÉ minh h·ªça.
    """
    
    try:
        response = model.generate_content(prompt)
        return response.text, total_comments
    except Exception as e:
        return f"L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}", 0

def process_multiple_chunks(title, content, comment_chunks, question, total_comments):
    """
    X·ª≠ l√Ω nhi·ªÅu chunks v√† t·ªïng h·ª£p k·∫øt qu·∫£
    """
    try:
        # X·ª≠ l√Ω t·ª´ng chunk
        chunk_analyses = []
        
        for i, chunk in enumerate(comment_chunks):
            chunk_context = f"""
            Ti√™u ƒë·ªÅ thread: {title}
            
            N·ªôi dung ch√≠nh:
            {content}
            
            Ph·∫ßn b√¨nh lu·∫≠n {i+1}/{len(comment_chunks)} (t·ª´ comment {i*50+1} ƒë·∫øn {i*50+len(chunk)}):
            """
            
            # Th√™m comments c·ªßa chunk n√†y
            for j, comment in enumerate(chunk):
                if hasattr(comment, 'reacts'):
                    reacts = comment.reacts
                    text = comment.text
                    link = comment.link
                else:
                    reacts = comment.get('reacts', 0)
                    text = comment.get('text', '')
                    link = comment.get('link', '')
                
                # C·∫Øt text comment n·∫øu qu√° d√†i
                if len(text) > 2000:
                    text = truncate_text(text, 2000)
                
                chunk_context += f"\nB√¨nh lu·∫≠n {i*200+j+1} ({reacts} reacts) - Link: {link}:\n{text}\n"
            
            # Ph√¢n t√≠ch chunk n√†y
            chunk_prompt = f"""
            B·∫°n l√† m·ªôt AI assistant chuy√™n ph√¢n t√≠ch v√† tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ c√°c thread tr√™n di·ªÖn ƒë√†n VOZ.
            
            {chunk_context}
            
            C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {question}
            
            H√£y ph√¢n t√≠ch ph·∫ßn b√¨nh lu·∫≠n n√†y v√† ƒë∆∞a ra nh·ªØng ƒëi·ªÉm ch√≠nh li√™n quan ƒë·∫øn c√¢u h·ªèi.
            Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·∫≠p trung v√†o nh·ªØng th√¥ng tin quan tr·ªçng nh·∫•t.
            """
            
            response = model.generate_content(chunk_prompt)
            chunk_analyses.append(response.text)
        
        # T·ªïng h·ª£p k·∫øt qu·∫£ t·ª´ t·∫•t c·∫£ chunks
        summary_prompt = f"""
        B·∫°n l√† m·ªôt AI assistant chuy√™n t·ªïng h·ª£p v√† ph√¢n t√≠ch th√¥ng tin.
        
        D∆∞·ªõi ƒë√¢y l√† c√°c ph√¢n t√≠ch t·ª´ {len(comment_chunks)} ph·∫ßn kh√°c nhau c·ªßa m·ªôt thread VOZ:
        
        {chr(10).join([f"**Ph·∫ßn {i+1}:**{chr(10)}{analysis}" for i, analysis in enumerate(chunk_analyses)])}
        
        C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {question}
        
        **Y√™u c·∫ßu:**
        - T·ªïng h·ª£p t·∫•t c·∫£ th√¥ng tin tr√™n th√†nh m·ªôt c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh
        - S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng Markdown ƒë·ªÉ tr√¨nh b√†y
        - T·∫°o c√°c ti√™u ƒë·ªÅ r√µ r√†ng v·ªõi ##
        - S·ª≠ d·ª•ng danh s√°ch c√≥ d·∫•u ƒë·∫ßu d√≤ng (-) cho c√°c ƒëi·ªÉm ch√≠nh
        - S·ª≠ d·ª•ng **bold** cho c√°c t·ª´ kh√≥a quan tr·ªçng
        - S·ª≠ d·ª•ng > cho c√°c tr√≠ch d·∫´n t·ª´ b√¨nh lu·∫≠n c·ª• th·ªÉ
        - S·ª≠ d·ª•ng `code` cho c√°c thu·∫≠t ng·ªØ k·ªπ thu·∫≠t
        - **Khi tr√≠ch d·∫´n comment:** S·ª≠ d·ª•ng format: > [N·ªôi dung comment](Link comment)
        
        Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, r√µ r√†ng v√† c√≥ c·∫•u tr√∫c. T·ªïng h·ª£p th√¥ng tin t·ª´ t·∫•t c·∫£ c√°c ph·∫ßn ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi to√†n di·ªán.
        """
        
        final_response = model.generate_content(summary_prompt)
        return final_response.text, total_comments
        
    except Exception as e:
        return f"L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}", 0

def process_single_post(url, db, News, Comment, AIAnalysis, flask_app, progress_callback=None):
    try:
        news_title, news_content, top_comments = get_top_comments(url, num_comments=5, progress_callback=progress_callback)
        title = news_title if news_title else url
        save_news_to_db(url, title, news_content, top_comments, ai_analysis=None, db=db, News=News, Comment=Comment, AIAnalysis=AIAnalysis, flask_app=flask_app)
    except Exception as e:
        print(f"Error processing URL: {e}")

def process_trending_posts(db, News, Comment, AIAnalysis, flask_app):
    url = "https://voz.vn/f/%C4%90iem-bao.33/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    trending_block = soup.find("div", {"data-widget-key": "forum_view__trending_content"})
    if trending_block:
        items = trending_block.find_all("li", class_="block-row")
        for item in items:
            title_tag = item.find("div", class_="contentRow-main").find("a")
            title = title_tag.text.strip()
            link = "https://voz.vn" + title_tag.get("href")
            replies_tag = item.find("div", class_="contentRow-minor", string=lambda text: text and "Replies:" in text)
            if replies_tag:
                replies_text = replies_tag.get_text(strip=True)
                replies = replies_text.replace("Replies:", "").strip()
            else:
                replies = "0"
            news_content, top_comments = get_top_comments(link, num_comments=5)
            save_news_to_db(link, title, news_content, top_comments, ai_analysis=None, db=db, News=News, Comment=Comment, AIAnalysis=AIAnalysis, flask_app=flask_app)
    else:
        print("Kh√¥ng t√¨m th·∫•y trending content!")

def save_news_to_db(url, title, news_content, comments, ai_analysis, db, News, Comment, AIAnalysis, flask_app):
    with flask_app.app_context():
        print(f"DEBUG: Saving URL to DB: {url}")
        news = News.query.filter_by(url=url).first()
        if not news:
            news = News(url=url, title=title, content=news_content)
            db.session.add(news)
            db.session.commit()
            print(f"DEBUG: Created new news entry with URL: {url}")
        else:
            print(f"DEBUG: Found existing news entry with URL: {url}")
        Comment.query.filter_by(news_id=news.id).delete()
        for c in comments:
            comment = Comment(
                news_id=news.id,
                reacts=c['reacts'],
                text=c['text'],
                link=c['link'],
                is_positive=c.get('is_positive'),
                created_at=c.get('date') if c.get('date') else None
            )
            db.session.add(comment)
        db.session.commit()

def is_positive_comment(reaction_ul):
    if not reaction_ul:
        return None
    imgs = reaction_ul.find_all("img", class_="reaction-image")
    if not imgs:
        return None
    first_title = imgs[0].get("title")
    if first_title == "∆Øng":
        return True
    if first_title == "G·∫°ch":
        return False
    return None

def get_forum_threads(News, db, flask_app=None):
    url = "https://voz.vn/f/%C4%90iem-bao.33/"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"}
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    list_threads = soup.find("div", class_="js-threadList")
    
    if list_threads:
        # Get all processed URLs from database
        processed_urls = set()
        if flask_app:
            with flask_app.app_context():
                processed_news = News.query.all()
                for news in processed_news:
                    processed_urls.add(news.url)
                print(f"DEBUG: Processed URLs in DB: {processed_urls}")
        
        # Debug: Print the HTML structure to see what we're working with
        print(f"DEBUG: HTML structure preview: {str(list_threads)[:500]}...")
        
        # Try different selectors to find thread items
        thread_items = list_threads.find_all("li", class_="structItem")
        if not thread_items:
            thread_items = list_threads.find_all("div", class_="structItem")
        if not thread_items:
            thread_items = list_threads.find_all("article", class_="structItem")
        if not thread_items:
            thread_items = list_threads.find_all("div", class_="structItem--thread")
        if not thread_items:
            thread_items = list_threads.find_all("li")  # Any li elements
        if not thread_items:
            thread_items = list_threads.find_all("div")  # Any div elements
            
        print(f"DEBUG: Found {len(thread_items)} thread items")
        if thread_items:
            print(f"DEBUG: First item classes: {thread_items[0].get('class', [])}")
            print(f"DEBUG: First item HTML: {str(thread_items[0])[:300]}...")
        
        # Extract thread items with their reply counts for sorting
        thread_data = []
        for item in thread_items:
            # Try different ways to find the title link - focus on thread links, not user links
            title_link = None
            
            # Look for the main thread link
            main_cell = item.find("div", class_="structItem-cell--main")
            if main_cell:
                # Find the thread title link
                title_link = main_cell.find("a", class_="fauxBlockLink-link")
                if not title_link:
                    title_link = main_cell.find("a", class_="structItem-title")
                if not title_link:
                    # Look for any link that contains /t/ (thread pattern)
                    all_links = main_cell.find_all("a", href=True)
                    for link in all_links:
                        href = link.get("href", "")
                        if "/t/" in href and not href.startswith("/u/"):
                            title_link = link
                            break
            
            if title_link:
                thread_url = "https://voz.vn" + title_link.get("href")
                print(f"DEBUG: Checking thread URL: {thread_url}")
                print(f"DEBUG: Is in processed_urls? {thread_url in processed_urls}")
                
                # Get reply count
                reply_count = 0
                meta_cell = item.find("div", class_="structItem-cell--meta")
                if meta_cell:
                    # Look for reply count in various formats
                    reply_text = meta_cell.get_text()
                    # Try to extract number from text like "Replies: 123" or "123 replies"
                    import re
                    reply_match = re.search(r'(\d+)', reply_text)
                    if reply_match:
                        reply_count = int(reply_match.group(1))
                
                thread_data.append({
                    'item': item,
                    'url': thread_url,
                    'reply_count': reply_count,
                    'is_processed': thread_url in processed_urls
                })
            else:
                print(f"DEBUG: Could not find title link in item")
                # Debug: print all links in this item
                all_links = item.find_all("a", href=True)
                for link in all_links:
                    href = link.get("href", "")
                    print(f"DEBUG: Found link: {href}")
        
        # Sort by reply count (descending)
        thread_data.sort(key=lambda x: x['reply_count'], reverse=True)
        
        # Clear the original list and add sorted items
        list_threads.clear()
        for thread_info in thread_data:
            item = thread_info['item']
            
            # Mark as processed if needed
            if thread_info['is_processed']:
                print(f"DEBUG: Marking as processed: {thread_info['url']}")
                # Add processed indicator
                processed_indicator = soup.new_tag("div")
                processed_indicator["style"] = "background: #4caf50; color: white; padding: 2px 6px; border-radius: 3px; font-size: 10px; margin-top: 4px; display: inline-block;"
                processed_indicator.string = "‚úì ƒê√£ x·ª≠ l√Ω"
                
                # Add to the title area
                title_cell = item.find("div", class_="structItem-cell--main")
                if title_cell:
                    title_cell.append(processed_indicator)
                
                # Also fade the entire item
                if "style" in item.attrs:
                    item["style"] += "; opacity: 0.6;"
                else:
                    item["style"] = "opacity: 0.6;"
            
            # Add reply count indicator
            reply_indicator = soup.new_tag("div")
            reply_indicator["style"] = "background: #007bff; color: white; padding: 2px 6px; border-radius: 3px; font-size: 10px; margin-top: 4px; display: inline-block; margin-left: 8px;"
            reply_indicator.string = f"üí¨ {thread_info['reply_count']}"
            
            # Add to the title area
            title_cell = item.find("div", class_="structItem-cell--main")
            if title_cell:
                title_cell.append(reply_indicator)
            
            list_threads.append(item)
        
        return str(list_threads)
    return "" 